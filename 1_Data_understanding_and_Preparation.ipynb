{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from autocorrect import Speller\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "tqdm.pandas()\n",
    "\n",
    "RANDOM_STATE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRISP-DM lifecycle:\n",
    "1. Business Understanding:\n",
    "• Understand the project's objectives and business goals, and define the problem you aim\n",
    "to solve with data mining.\n",
    "2. Data Understanding:\n",
    "• Gather and explore data to gain insights into its quality, structure, and relevance to the\n",
    "problem. This step often involves data collection, data profiling, and initial data analysis.\n",
    "3. Data Preparation:\n",
    "• Preprocess and clean the data to make it suitable for modeling. Tasks include handling\n",
    "missing values, dealing with outliers, and transforming variables.\n",
    "4. Modelling:\n",
    "• Select and apply appropriate data mining and machine learning techniques to build and evaluate\n",
    "models. This step includes selecting algorithms, and training models, and assessing their\n",
    "performance.\n",
    "5. Evaluation:\n",
    "• Assess the quality and effectiveness of the models developed in the previous step. This involves\n",
    "metrics, cross-validation, and comparing model performance against the project's goals.\n",
    "6. Deployment:\n",
    "• Deploy the selected model(s) into the production environment for real-world use. This step may\n",
    "involve integrating the model into business processes and monitoring its performance over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining: CA2\n",
    "by Peter and Jonas Wortmann\n",
    "## 1. Data understanding and Preparation\n",
    "Sentiment Analysis (SA) is a sub field of text mining and is used to extract the opinions, sentiments and subjectivity of text [[1]](#1). This assignment aims to classify the sentiment of user reviews based on their text features.\n",
    "\n",
    "<a id=\"1\">[1]</a> \n",
    "Walaa Medhat, Ahmed Hassan, Hoda Korashy,\n",
    "Sentiment analysis algorithms and applications: A survey,\n",
    "Ain Shams Engineering Journal,\n",
    "Volume 5, Issue 4,\n",
    "2014,\n",
    "Pages 1093-1113,\n",
    "ISSN 2090-4479,\n",
    "https://doi.org/10.1016/j.asej.2014.04.011.\n",
    "(https://www.sciencedirect.com/science/article/pii/S2090447914000550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a one of the best apps acording to a b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a pretty good version of the game for ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this is a really cool game. there are a bunch ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is a silly game and can be frustrating, b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is a terrific game on any pad. Hrs of fun...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  Positive\n",
       "0  This is a one of the best apps acording to a b...         1\n",
       "1  This is a pretty good version of the game for ...         1\n",
       "2  this is a really cool game. there are a bunch ...         1\n",
       "3  This is a silly game and can be frustrating, b...         1\n",
       "4  This is a terrific game on any pad. Hrs of fun...         1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_reviews = pd.read_csv(\"data/amazon.csv\")\n",
    "with open(\"StopWords.txt\", \"r\") as f:\n",
    "    stopwords = f.read().splitlines() \n",
    "user_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   reviewText  20000 non-null  object\n",
      " 1   Positive    20000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 312.6+ KB\n"
     ]
    }
   ],
   "source": [
    "user_reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Positive'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_reviews.Positive.value_counts().plot(kind=\"bar\", rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 20,000 rows. Each row consists of a user review and it's corresponding sentiments. The labels are encoded as integers, 1 = positive and 0 = negative. The dataset has unbalanced classes as you can see in the plot above.\n",
    "\n",
    "### Data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a one of the best apps acording to a bunch of people and I agree it has bombs eggs pigs TNT king pigs and realustic stuff'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_reviews.iloc[0].reviewText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this. is fun an time consuming. works great on my kindle fire I really like this game so does my btother'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_reviews.iloc[11].reviewText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reviews contain spelling mistakes, for example \"btother\", \"realustic\" and \"acording\" as you can see in the texts above. \\\n",
    "To avoid spelling mistakes the python library autocorrect is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a one of the best apps according to a bunch of people and I agree it has bombs eggs pigs NT king pigs and realistic stuff'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell = Speller(lang='en')\n",
    "spell(user_reviews.iloc[0].reviewText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this. is fun an time consuming. works great on my kindly fire I really like this game so does my brother'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell(user_reviews.iloc[11].reviewText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if the autocorrect library properly corrected all the mentioned mistakes, it wrongly corrected \"TNT\" to \"NT\" and \"kindle fire\" to \"kindly fire\" so this autocorrection is not used in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"His is a one of the best apes according to a bunch of people and I agree it has bombs eggs pigs TNT king pigs and realistic stuff\")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "TextBlob(user_reviews.iloc[0].reviewText).correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"this. is fun an time consuming. works great on my kindle fire I really like this game so does my brother\")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(user_reviews.iloc[11].reviewText).correct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextBlob also makes mistakes by falsely correcting \"apps\" to \"apes\". Since auto-spelling correction is a challenging task and leads to misleading results as seen above, no spelling correction is applied for the review texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reviews also contain letters that are repeated multiple times, like \"soooooo\". Tokens like \"sooooo\" likely occur just once in the corpus and the meaning of the token can hardly be extracted. Further, missing whitespaces, e. g. \"[...] make what this is all about.It's not relevant [...]\", between the punctuation and the new sentence makes the sentence recognition challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning\n",
    "[[2]](#2) compare punctuation removal in sentiment analysis scenarios of english and turkish reviews and come to the conclusion to keep punctuation and stopwords because they contain meaningful information in terms of their sentiment. [[3]](#3) also identifies punctuation as a good feature for Twitter sentiment analysis. Therefore, the stopwords and the punctuation are kept in the corpus, only lowercasing is applied in the cleaning process.\n",
    "\n",
    "### Tokenization & Embeddings\n",
    "In [[4]](#4), Transformer models outperform other machine learning models (i.e. LSTM, BoT, CNN) in sentiment analysis. Transformer is one of the most robust state-of-the-art approach in NLP. In this section we use the Transformer model BERT to tokenize the reviews to obtain BERT's 768-dimensional embeddings as input for our model.\n",
    "\n",
    "<a id=\"2\">[2]</a> Parlar, Tuba & Özel, Selma & Song, Fei. (2019). Analysis of data pre-processing methods for the sentiment analysis of reviews. Computer Science. 20. 123. 10.7494/csci.2019.20.1.3097. \\\n",
    "<a id=\"3\">[3]</a>  Koto, F., Adriani, M. (2015). A Comparative Study on Twitter Sentiment Analysis: Which Features are Good?. In: Biemann, C., Handschuh, S., Freitas, A., Meziane, F., Métais, E. (eds) Natural Language Processing and Information Systems. NLDB 2015. Lecture Notes in Computer Science(), vol 9103. Springer, Cham. https://doi.org/10.1007/978-3-319-19581-0_46 \\\n",
    "<a id=\"4\">[4]</a> Balci, S., Demirci, G.M., Demirhan, H., Sarp, S. (2022). Sentiment Analysis Using State of the Art Machine Learning Techniques. In: Biele, C., Kacprzyk, J., Kopeć, W., Owsiński, J.W., Romanowski, A., Sikorski, M. (eds) Digital Interaction and Machine Intelligence. MIDI 2021. Lecture Notes in Networks and Systems, vol 440. Springer, Cham. https://doi.org/10.1007/978-3-031-11432-8_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 185/20000 [00:14<25:30, 12.94it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# get embeddings for each document\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m user_reviews[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43muser_reviews\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreviewText\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_bert_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jonas\\anaconda3\\envs\\dm_ca2\\Lib\\site-packages\\tqdm\\std.py:920\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[1;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 920\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_function\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    922\u001b[0m     t\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\jonas\\anaconda3\\envs\\dm_ca2\\Lib\\site-packages\\pandas\\core\\series.py:4764\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4630\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4631\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4636\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4637\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4638\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4639\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4640\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4755\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4756\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4758\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4762\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4764\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jonas\\anaconda3\\envs\\dm_ca2\\Lib\\site-packages\\pandas\\core\\apply.py:1209\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jonas\\anaconda3\\envs\\dm_ca2\\Lib\\site-packages\\pandas\\core\\apply.py:1289\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1287\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1289\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1295\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\jonas\\anaconda3\\envs\\dm_ca2\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jonas\\anaconda3\\envs\\dm_ca2\\Lib\\site-packages\\pandas\\core\\algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1818\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2926\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\jonas\\anaconda3\\envs\\dm_ca2\\Lib\\site-packages\\pandas\\core\\apply.py:1278\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard.<locals>.curried\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcurried\u001b[39m(x):\n\u001b[1;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jonas\\anaconda3\\envs\\dm_ca2\\Lib\\site-packages\\tqdm\\std.py:915\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    910\u001b[0m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[0;32m    911\u001b[0m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[0;32m    912\u001b[0m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[0;32m    913\u001b[0m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[0;32m    914\u001b[0m     t\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m, in \u001b[0;36mget_bert_embedding\u001b[1;34m(inputs, model)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_bert_embedding\u001b[39m(inputs, model):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 12\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\jonas\\anaconda3\\envs\\dm_ca2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jonas\\anaconda3\\envs\\dm_ca2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jonas\\anaconda3\\envs\\dm_ca2\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1012\u001b[0m )\n\u001b[1;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jonas\\anaconda3\\envs\\dm_ca2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jonas\\anaconda3\\envs\\dm_ca2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jonas\\anaconda3\\envs\\dm_ca2\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    598\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    604\u001b[0m         output_attentions,\n\u001b[0;32m    605\u001b[0m     )\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\jonas\\anaconda3\\envs\\dm_ca2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jonas\\anaconda3\\envs\\dm_ca2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jonas\\anaconda3\\envs\\dm_ca2\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    536\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    537\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 539\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    542\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    544\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jonas\\anaconda3\\envs\\dm_ca2\\Lib\\site-packages\\transformers\\pytorch_utils.py:242\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jonas\\anaconda3\\envs\\dm_ca2\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:551\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m--> 551\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    552\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\jonas\\anaconda3\\envs\\dm_ca2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jonas\\anaconda3\\envs\\dm_ca2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jonas\\anaconda3\\envs\\dm_ca2\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:451\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 451\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\jonas\\anaconda3\\envs\\dm_ca2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jonas\\anaconda3\\envs\\dm_ca2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jonas\\anaconda3\\envs\\dm_ca2\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def tokenize(text, model, tokenizer):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    return tokens\n",
    "\n",
    "def get_bert_embedding(inputs, model):\n",
    "    # inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# get embeddings for each document\n",
    "user_reviews[\"bert_embedding\"] = user_reviews[\"reviewText\"].apply(tokenize, model=model, tokenizer=tokenizer).progress_apply(get_bert_embedding, model=model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_reviews[\"bert_embedding\"].to_pickle(\"data/embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size:  14000\n",
      "Test data size:  6000\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into train (70%) and test (30%)\n",
    "train_indices = user_reviews.sample(int(user_reviews.shape[0]*0.7), random_state=RANDOM_STATE).index\n",
    "train_df = user_reviews.loc[train_indices]\n",
    "test_df = user_reviews.drop(train_indices)\n",
    "\n",
    "print(\"Training data size: \", train_df.shape[0])\n",
    "print(\"Test data size: \", test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonas\\anaconda3\\envs\\dm_ca2\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tokenize\n",
    "encoder_train = tokenizer.batch_encode_plus(train_df[\"reviewText\"].tolist(),\n",
    "                                           add_special_tokens = True,\n",
    "                                           pad_to_max_length = True,\n",
    "                                           max_length = 256,\n",
    "                                           return_tensors = 'pt')\n",
    "\n",
    "\n",
    "\n",
    "encoder_test = tokenizer.batch_encode_plus(test_df[\"reviewText\"].tolist(),\n",
    "                                           add_special_tokens = True,\n",
    "                                           pad_to_max_length = True,\n",
    "                                           max_length = 256,\n",
    "                                           return_tensors = 'pt')\n",
    "\n",
    "# create a TensorDataset\n",
    "input_ids_train = encoder_train[\"input_ids\"]\n",
    "input_ids_test = encoder_test[\"input_ids\"]\n",
    "attention_mask_train = encoder_train[\"attention_mask\"]\n",
    "attention_mask_test = encoder_test[\"attention_mask\"]\n",
    "\n",
    "labels_train = torch.tensor(train_df[\"Positive\"].to_numpy()).float()\n",
    "labels_test = torch.tensor(test_df[\"Positive\"].to_numpy()).float()\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(input_ids_train,attention_mask_train, labels_train)\n",
    "test_dataset = TensorDataset(input_ids_test,attention_mask_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train.pkl', 'wb') as f:\n",
    "    pickle.dump(train_dataset, f)\n",
    "\n",
    "with open(\"data/test.pkl\", 'wb') as f:\n",
    "    pickle.dump(test_dataset, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dm_ca1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
